[{"id":0,"title":"Character Recognizer - A CNN Model (WIP)","author":"Mitsuki Youko","date_created":"2025-04-19","tag":"projects","blurb":"The process of training a CNN (in gradual steps) to recognize my 5 OCs.","content":"\n## Overview\n\nAs I'm looking to build my portfolio as a \"Jack of All Trades\", AI was a topic that was recommended to me.\n\nAfter some research, it seemed that a good beginner project would be the [Cats Vs Dogs Project](https://www.kaggle.com/c/dogs-vs-cats), where you train a model from a clean slate where it knows nothing about anything, and you teach it to differentiate between Cats and Dogs.\n\nThe idea is that you have your cat images in a folder labeled Cats and your dog images in a folder labeled Dogs and you create a Convolutional Neural Network (CNN) composed of different layers so that it keeps looping through your images to learn the characteristics of a cat and dog, etc.\n\nSince I am very passionate about my [personal project](https://mitsukiyouko555.wixsite.com/portfolio/personal-project), I always take any chance I could to involve my characters in whatever non-personal project thing I'm working on - so I figured, why not create a CNN that is able to differentiate between my 5 Main Characters?\n\n---\n\n## Issues: \n\n### 1. Small Dataset\n\nThe Cats vs Dogs dataset is HUGE at over 25,000 Images to train on. I draw my ocs pretty inconsistently (in terms of who I draw more) lol... I mainly draw Maxus and Hydra and don't draw the others as much.\n\nAs such, I will have to draw an even amount of art per character.\n\nBut to reduce the time that it takes, I'm taking a ground up approach.\n\nFirst, I will train the CNN on the images of ONLY my characters' eyes - no background/transparent background.\n\nThen I will validate it by drawing a few more \"eyes only\" drawings and see if the model can correctly predict which set of eyes belong to which character.\n\nThen I will do the same with just the headshots of my characters, and then separately, do the same for full body shots.\n\nAs a final step, I will include multi-character images and have the model predict which character(s) are in the image.\n\n\n### 2. Image Size\n\nWhen training neural networks, it is strongly advised that the images be in the same size.\n\nTherefore, for the headshots, the images will be wider than they are tall but once I've decided on a fixed size, I will draw all the eyes-only pics in that one size only.\n\nThe same idea applies for the headshots which will be square.\n\nThe Full Body Images will be taller than they are wide, but of course, at the same consistent image size.\n\nThe Multi-Lable ones (the ones where the will be one or more characters in an image) will be a standard HD size (1920x1080) and will be shrunk down in correct proportions, if needed, when training the data.\n\n### 3. Image Variety\n\nIts important for there to be variety in terms of the background, otherwise the model might associate certain backgrounds with certain characters.\n\nSo I will need to build a dataset that has characters with no background, characters with a solid color or gradient/simple background, and characters with complex backgrounds.\n\nI also had the idea of animating some of these images - specifically for the eyes and headshots and MAYBE for a few of the fullbody ones as well and exporting it as png sequence so that it can have more data without me doing too much extra work.\n\n---\n\n### Timeline\n\nThis project will take at LEAST a few months if not more - but mainly because I need to create my dataset. If I were grabbing images off the internet, it would be way faster.. But because I have what eventually amounts to at least a hundred images to draw, (and I draw slow), it would take a while.\n\nWith Gemini's help and guidance, I suspect the actual coding of it is not that hard, but going through and typing it out line by line, tweaking it and really understanding what the code does and experimenting with it is going to be fun!\n\nI anticipate coming out of this project with a bunch of art created, as well as a cool CNN Model that can tell my characters apart.\n\n---\n\n### Research\n\n### Issues:\n1. imports not importing (specifically keras.) \n    - Fix: [https://stackoverflow.com/questions/78998877/import-tensorflow-keras-callbacks-import-earlystopping-modelcheckpoint-could-n](https://stackoverflow.com/questions/78998877/import-tensorflow-keras-callbacks-import-earlystopping-modelcheckpoint-could-n)\n    - Calling Keras via from `keras.src.callbacks import xyz` instead of keras.callbacks is what fixed it.\n\n#### Phase 1 - Eyes Only\n\n\n#### Phase 2 - Headshot Only\n\n\n\n#### Phase 3 - Full Body Only\n\n\n\n#### Phase 4 - Multi-Label (Multiple Characters)\n\n\n### Outcome and Results","folderName":"CharacterRecognizer"},{"id":1,"title":"Conceptual Technical Writing - Minimizer","author":"Mitsuki Youko","date_created":"2025-05-31","tag":"techwriting","blurb":"A blogpost showcasing the application of my technical writing skills to a conceptual mechanism.","content":"\n# Premise\n\nThis is a technical writing sample (for portfolio purposes), revolving around a conceptual Futuristic Object Compression Mechanism, used in the year 3050, called the \"Minimizer\" from my [Personal Project](https://mitsukiyouko555.wixsite.com/portfolio/personal-project).\n\n# Technical Writing\n\n### Compression\n\n1. Activate your holo\n\n2. Open the compression app\n\n3. Click scan and point your holo's camera towards the object. Once scanned, the item is linked and absorbed into your holo for compression purposes.\n    \n\t1. If multiple objects get scanned, select the object(s) you want to compress, ignoring those you don't\n\t2. If compressing multiple objects, see if you want to compress them all into one cube or if you want to compress them into individual cubes\n\t3. if you decide to cancel the compression, click cancel and your item will be released from the holo and be physically accessible again.\n4. Once the item(s) have been selected, select the compression size (customizable using a circle control)\n5. Select a compression shape (Cube, Sphere, Diamond, Custom, etc.)\n\t1. A cubed compression looks like this\n\t2. A Diamond Compression looks like this\n\t3. A Sphere Compression looks like this\n\t4. To create a custom compression shape, click Custom and a modeling tool will pop up.\n\t\t1. If you've saved custom shapes before, in the top right hand corner, click the hamburger menu and click \"saved shapes\". Otherwise, proceed with modeling a custom shape.\n\t\t\t1. You have the option of modeling it yourself from scratch\n\t\t\t2. or prompting AI to create a shape for you.\n\t\t\t\t1. you can tell it how many variations to generate and select the ones you like\n\t\t\t\t2. you can also describe the use case for your compression (ie fits in a box shaped like this (with a diamond shaped mold/inset), etc.)\n\t\t2. If you want to model it yourself, \\<insert blender-like modeling directions here>\n\t\t3. Once you have modeled your custom shape, click save\n\t\t\t1. you can also save the shape as a template for future use\n6. click compress\n7. A compression status circle will show in real time as it compresses.\n8. Once compressed, the status will show as complete\n9. You can choose one of these Security options\n\t1. Unsecured\n\t\t1. Anyone who wants to uncompress it can do so\n\t2. Password\n\t\t1. a window will pop up for you to put in a password\n\t\t2. the item can only be uncompressed by whoever puts in the right password\n\t3. Fingerprint\n\t\t1. a window will pop up on your holo's keyboard panel (your keyboard will be replaced with a fingerprint scanner for all 10 fingers) to scan your fingerprints\n\t4. Soul Print\n\t\t1. Only users with the soul print of the holo (aka only the user who compressed it can uncompress it)\n\t\t2. a secret key is generated in the background and stored on the compressed item(s)\n10. \n11. Click Export and The holo will present the compressed item to you (it comes out of the holo, between the user and the holo.)\n\t1. on opposite sides of the item, there is a half circle. so long as those remain as is, the item will remain compressed\n\n### Decompression\n\n1. To Decompress, it depends on the level of security that was chosen during the compression process.\n\t1. unsecured\n\t\t1. Anyone can decompress it\n\t2. password\n\t\t1. enter the password and only then will you be given access to decompress it\n\t3. fingerprint\n\t\t1. the item itself produces ITS OWN holo of which scans your fingerprints. If it matches, it allows you to decompress\n\t4. soul print\n\t\t1. the item connects its holo to your holo in the background and if their keys match, you are allowed to decompress it\n2. The actual decompression process is quite simple. take the item and twist it til the 2 half circles form a full circle and it will decompress back into the item's original size.\n3. To recompress an item, follow the compression steps again.\n\n\n\n\n<!-- NOTE: \n\n0. The blurb is a 10 word blurb describing briefly what the blogpost is about\n\n1. When making a new page, remember to run this to compile the pages:\nnpm run server\n\n2. Then double check with\nnpm run build\n\n3. Then do your git stuff\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\n\n4. Then deploy it with:\nnpm run deploy\n\n--- -->\n\n<!-- npm run server\nnpm run build\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\nnpm run deploy -->","folderName":"ConceptualTechnicalWriting-ObjectCompression"},{"id":2,"title":"My Favorite Obsidian Plugins (WIP)","author":"Mitsuki Youko","date_created":"2025-06-01","tag":"misc","blurb":"My favorite Obsidian Plugins and what I use them for.","content":"\n# Heading\n\nContent\n\nTag Categories:\n\n    Technical Writing - techwriting\n        - These are tutorials / How to's (INDIVIDUAL TOPICS)\n    Projects - projects\n        - Why should it have its own category instead of being lumped with technical writing? probs cus its easier to find..\n    Cy & Den - cyandden\n        - Cy and Den manga? but what would the topics be?\n    Ethical Hacking - ethicalhacking\n        - Ethical hacking How to's\n    Scripts - scripts\n        - Scripts that i made and script breakdowns\n    Misc - misc\n        - Behind-the-Scenes & Personal Development, Explorations & Experiments, Industry News & Analysis (Outside of Specific How-Tos)\n\nNOTE: \n\n0. The blurb is a 10 word blurb describing briefly what the blogpost is about\n\n1. When making a new page, remember to run this to compile the pages:\nnpm run server\n\n2. Then double check with\nnpm run build\n\n3. Then do your git stuff\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\n\n4. Then deploy it with:\nnpm run deploy\n\n---\n\nnpm run server\nnpm run build\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\nnpm run deploy","folderName":"FavObsidianPlugins"},{"id":3,"title":"Hack The Box - Meow (WIP)","author":"Mitsuki Youko","date_created":"2025-06-01","tag":"ethicalhacking","blurb":"A short walkthrough detailing the process of pwning Hack The Box's Meow Machine","content":"\n# Heading\n\nContent\n\nTag Categories:\n\n    Technical Writing - techwriting\n        - These are tutorials / How to's (INDIVIDUAL TOPICS)\n    Projects - projects\n        - Why should it have its own category instead of being lumped with technical writing? probs cus its easier to find..\n    Cy & Den - cyandden\n        - Cy and Den manga? but what would the topics be?\n    Ethical Hacking - ethicalhacking\n        - Ethical hacking How to's\n    Scripts - scripts\n        - Scripts that i made and script breakdowns\n    Misc - misc\n        - Behind-the-Scenes & Personal Development, Explorations & Experiments, Industry News & Analysis (Outside of Specific How-Tos)\n\nNOTE: \n\n0. The blurb is a 10 word blurb describing briefly what the blogpost is about\n\n1. When making a new page, remember to run this to compile the pages:\nnpm run server\n\n2. Then double check with\nnpm run build\n\n3. Then do your git stuff\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\n\n4. Then deploy it with:\nnpm run deploy\n\n---\n\nnpm run server\nnpm run build\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\nnpm run deploy","folderName":"HackTheBoxMeow"},{"id":4,"title":"How I Almost Got Hacked (WIP)","author":"Mitsuki Youko","date_created":"2025-03-30","tag":"cyandden","blurb":"A cautionary tale about how, despite knowing the signs, I still ALMOST got hacked.","content":"\n# Infographic goes here... (WIP)","folderName":"HowIAlmostGotHacked"},{"id":5,"title":"Introducing Cy and Den","author":"Mitsuki Youko","date_created":"2025-03-27","tag":"misc","blurb":"Introducing Cyanscythe (Cy) and Goldenmage (Den).","content":"\n### What is / Who are CyanScythe (Cy) and GoldenMage (Den)?\n\nI love drawing and creating characters and figured... Why not mash that and what I learn in Ethical Hacking together?\nThat gave me the idea of drawing infographics whenever I learned something that could be infographic-worthy.\n \nSince I needed a name for my blog I just randomly came up with the word CyanScythe and created a character based off of that which I thought would be cool as an attacker type character. Then I thought \"Well attacker = red team.. so I'll need a \"defender\" and so I created GoldenMage.\n​\nIssue was that their color schemes were that of the opposing team.. but CyanScythe was such a cool name that I decided to make it so that she has magenta eyes and when she \"attacks\" with her scythe, it will turn red - thus giving her a red team vibe, despite the fact that her color scheme was cyan.\n \nDesign-wise, GoldenMage was a bit easier though I originally imagined his color scheme to be red and gold but since he is on the blue team, I gave him blue accessories. His powers are also blue to give off the blue team vibe.\n​\nNot only that, but a good theme here is that their designs can represent how important it is for red teamers to learn how to defend against cyber attacks and blue teamers to learn how attackers would attack, forming a good purple team - since they have colors of the opposing team.\n​\nIn the CyanScythe and GoldenMage Infographics, Cy will be presenting things from a Red Team Perspective while Den will be presenting things from a Blue Team Perspective.\n\nYou may also see a few comics about the both of them, if I happen to have a topic that can effectively be told in comic format.\n\nHope you enjoy the blog!\n\n---\n\n#### Cy's Character and Weapon Designs:\n\nWhen in Neutral mode, Cy's scythe is blue but it turns red when she is attacking.\n\n![Cy Neutral Mode](assets/content/IntroCyAndDen/img/Cy%20Neutral%20Mode.jpg)\n![Cy Attacker Mode](assets/content/IntroCyAndDen/img/Cy%20Attacker%20Mode.jpg)\n\n#### Den's Character and Weapons Design:\n\nDen's Barriers are not just for defensive purposes - he can use them to attack as well!\n\n![Den](assets/content/IntroCyAndDen/img/Den.jpg)\n![Den Weapon Design](assets/content/IntroCyAndDen/img/Den%20Weapons%20Design.png)\n","folderName":"IntroCyAndDen"},{"id":6,"title":"Pi Hole DNS Script","author":"Mitsuki Youko","date_created":"2025-04-19","tag":"scripts","blurb":"A Script to point your Linux Desktops to your Raspberry Pi's Pi Hole.","content":"\n## Note: This script assumes you have a Raspberry Pi and that you have Pi Hole Set up on your Raspberry Pi Already.\n\nThis script ensures that your Linux Desktop is pointed at your Pi Hole for DNS.\n\nIf you anticipate running this script for multiple machines, I would recommend deploying it via something like Ansible/Salt/Puppet or some other automation of that nature.\n\n[Here](https://github.com/mitsukiyouko555/saltstack), I used Saltstack to deploy it to my VMs.\n\nThis does not appear to work on Ubuntu SERVERS but it does work on Ubuntu Desktop, as well as Kali Linux.\n\nThe reason it does not work on Ubuntu servers is due to the difference in syntax when getting the connection name to show up.\n\n![PiDnsScript](assets/content/PiDnsScript/img/script.png)\n\n\n## Line By Line Breakdown:\n\n---\n\n### 1. #!/bin/bash\n\n^ This is what is known as the Shebang. It tells Linux what binary or command to use when running the program.\nIn this case, as we are coding in bash, this tells Linux that our bash is in the /bin/bash directory.\n\n---\n\n### 2. con=$(nmcli -t -f NAME connection show)\n\n![2](assets/content/PiDnsScript/img/2.jpg)\n\n^ The command shows the list of network connections that your machine has, such as eth0, for example..\nFor VMs, there's generally a few different connections.\n\nThis line is important because the connection NAME for each vm may vary and to point it at the DNS, we need the right name..\n\nFor example, one Ubuntu VM may have eth98 while another Ubuntu VM, while the OS is the same, may have a different name, such as eth07 - so this value cannot be hardcoded.\n\n---\n\n### 3. while IFS= read -r line; do\n\n^ This is similar to a while loop in that it loops though all the results and gets each line\n\n---\n\n### 4. #echo \"Connection: $line\"\n\n^ This was for testing purposes - to ensure that the results of the variable \"line\" was what I expected it to be.\n\n---\n\n### 5. nmcli con mod \"$line\" ipv4.ignore-auto-dns yes\n\n![5](assets/content/PiDnsScript/img/5.jpg)\n\n^ This line is modifying the output of the line that we got from running \"nmcli -t -f NAME connection show\" so it basically says for every connection that is being listed, Ignore the automatically assigned DNS (because we want it to point to the one we are setting manually.)\n\n---\n\n### 6. nmcli con mod \"$line\" ipv4.dns \"<Raspberry Pi's IP Address> 8.8.8.8 1.1.1.1\"\n\n![6](assets/content/PiDnsScript/img/6.jpg)\n\n^ This says modify the connection in the line variable and update the IPV4 DNS to point to our Raspberry Pi's IP address - thus making it use Pi Hole for the DNS.\n\nIn addition to that, 8.8.8.8 and 1.1.1.1 are Google and Cloudflare's respective DNS's.\n\nThese are there as fall backs so that if your Pi Hole is down, you can at least still connect to the internet via Google/Cloudflare's DNS.\n\n---\n\n### 7. done <<< \"$con\"\n\n^ This means once it's looped through all the connections and applied the steps above, the while loop can be exited.\n\n---\n\n### 8. service NetworkManager restart\n\n![8](assets/content/PiDnsScript/img/8.jpg)\n\n^ This restarts the Network Manager service so that the new DNS settings will reflect properly.\n\n---\n\n### 9. cat /etc/resolv.conf\n\n![9](assets/content/PiDnsScript/img/9.jpg)\n\n^ This shows the contents of the /etc/resolv.conf file, which is where the DNS info is stored.\n\nThis is so that we can confirm that the settings have been set properly.\n","folderName":"PiDnsScript"},{"id":7,"title":"Using rrsync with rsync (WIP)","author":"Mitsuki Youko","date_created":"2025-05-12","tag":"techwriting","blurb":"A guide on how to use the lesser documented rrsync with rsync for a secure, encapsulated way to collaborate remotely.","content":"\nWIP\n\n---\n\nnpm run server\nnpm run build\ngit add .\ngit commit -m \"update note\"\ngit push -u origin\nnpm run deploy\n\n\n---\n\nIf you accidentally commited a file that is too big, remove it with this and then recommit:\n\ngit filter-branch -f --tree-filter 'rm -f /path/to/file' HEAD --all\n","folderName":"RsyncAndRRsync"},{"id":8,"title":"Running Dog Animation Critique","author":"Mitsuki Youko","date_created":"2025-04-03","tag":"misc","blurb":"Running Dog Anim Critique for Redditor u/remyisacutie.","content":"\n### Overview\n\nThis blogpost is a bit different than my others as I will be breaking down my animation and composting process for [this reddit post](https://www.reddit.com/r/animation/comments/1jqdx7c/ik_this_is_wrong_but_how_do_i_fix_it/) by u/remyisacutie.\n\nHere is u/remyisacutie's original animation:\n\n![runningdog](assets/content/RunningDog/img/runningdog.gif)\n\nFYI: I am not a professional artist or animator. I'm a hobbyist so take my critique with however many grains of salt you'd like.\n\nNote: There are some large gifs and images in this Blogpost. If an image/gif looks like it's not loading, try refreshing the page and it should pop up...\n\nHere are my initial thoughts on it:\n\n1. Dog's eyes are too static, introduce some blink\n2. Belly flops around a bit toooo much.. its like his ribs are moving - seems slightly unnatural.. If he just has a fat belly, the flop should be lower. (imo)\n3. Legs are good but not quite on the ground\n4. No shadow\n5. Cloud moving in different directions.. i guess that kinda works but if there's wind thats impacting the clouds that much, the trees would be affected a little too..\n6. Ears, tail, and legs are actually quite good!\n7. Tongue looks like its being swung forward.. doesn't quite match the running direction...\n8. Tail is decent but it gets a little bit longer near the end.. (I struggle with that sometimes when I animate hair XD)\n9. Weight looks like it is on all 4 legs at the same time... I feel that when landing, weight should be on the front legs... this can probs be fixed simply by transforming and tilting the whole dog snout-side down just a tiny bit...\n10. Neck wrinkles are good but slightly off on timing it feels like..\n11. Looks like the path/ground is going z-axis yet the dog is running x axis which looks a bit.. strange i guess? but that might just be the placement cuz its not moving.. if the floor is moving, it would sell the thing a bit more...\n12. Imo the reason why it doesn't loop well is cus you probs need one more frame for some of the poses like the jump between the last and first frame is too big.\n13. Lighting.. The lighting changes too quickly on the dog like its running past a spotlight.. if its sunlight because its so far away, it wouldn't move on the dog that fast.\n14. Camera movement (maybe when landing, camera should follow the land a little - not TOO drastically but just a little movement helps)\n15. When looping always think back to HOW it loops. Do sketches first to see if it loops properly.. (before coloring or linearting). One trick is on the second half of the loop, use the frames in the first half in reversed order, then change some stuff in each so it doesn't just look like a copy\n\n---\n\n### Small Project Break Down..\n\n#### Animation Layers:\n1. Dog\n2. Shadow\n3. Foreground (Grass)\n4. Midground(ground and trees)\n    1. Make the ground long so that it you can move it x-axis. \n5. Background (clouds and mountains)\n6. Sky\n\n#### My Workflow for this critique demo:\n1. Sketch the dog running\n2. Sketch the dog's shadow\n3. Sketch the long bg on a different document. if animating, then composite it later. \n4. Lineart the bg\n5. Color the bg\n6. Lineart the dog\n7. Color the dog\n8. Color the Shadow\n9. Composite (Either baked in or after export.)\n\n#### I'll be making Two versions:\n1. Dog running with animated bg (BAKED IN Compositing)\n2. Dog running with animated bg (Compositing after Exporting)\n\n#### Composting tips:\n1. Export EACH animation layer separately so as to be able to add effects to them.. by layers I don't mean the dog's legs being on a different layer than its eyes... I mean Dog and its sublayers get exported as 1 png animation, the grass get exported as its own animation etc.. Anything you need to composite separately gets exported as its own png gif or image sequence.\n\n---\n\n### DEMO\n\nSo First thing's first.. gotta lay down the rough sketch of the dog model since its so long since I've drawn a dog.. (Had to rewatch the animal anatomy part of Marc Brunet's Digital Art School course that I bought a few years ago XD)\n\n![runningdogmodel](assets/content/RunningDog/img/Runningdogmodel.jpg)\n\nI used the straight line tool in CSP to make the line for the floor.\n\nThis is to make sure that when animating, the dog's paws won't go too high above or too low below the \"floor\". ideally it should land right on the line or as close to it as possible.\n\nI sketched the dog model just for reference and so I can make sure the animation stays on character.\n\nNow, so far drew a box around the dog so i can roughly tell where things are going to be placed...\n\n![dog-rectangle](assets/content/RunningDog/img/dog-rectangle.jpg)\n\nAnd then I animated the box to get the movement down..\n\n![blockout.gif](assets/content/RunningDog/img/blockout.gif)\n\n![roughplacement](assets/content/RunningDog/img/roughplacement.jpg)\n\nAs you can see, the frames are evenly spread out.. not very dynamic is it..?\n\nDon't worry, I will adjust the timing now after the rough movement has been placed..\n\nHere, I tend to play around - extend some frames, shorten others.. some people animate on 2's, 3's, 4's... that's way too much math for me loooll\n\nI animate based on feel.. so if you want to animate industry level stuff you might have to study how to animate on 2's etc. or whatever the industry uses :P\n\nI'm just a hobbyist and if it looks good to me, well that's really all I need XD\n\nSo after some re-timing, the frames now look like this:\n\n![frames_updated](assets/content/RunningDog/img/frames_updated.jpg)\n\nI also slightly adjusted the position and rotation of a few of the rectangles for a smoother flow..\n\n![blockout_framed](assets/content/RunningDog/img/blockout_framed.gif)\n\nSo the next phase of my rough sketch is going to be to move the bg around..\n\nWatch this excellent video on how to use Keyframes for Camera Movement in CSP:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8F9eJjDkvtE?si=l5YfQy6QEYK4ZD_p\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nIf you don't use CSP, you can try to see if the software you use supports camera movements.\n\nOtherwise, if you want to do your camera movements during compositing instead, make sure you make the canvas size like roughly 0.5-1 inches bigger so that when compositing, your bg won't get awkwardly cut off when you reach the edge of the canvas.\n\nHere, I have the background animation folders within a larger folder and applied the keyframes to said folder.\n\nThe rectangle \"dog\" was outside the \"camera\" folder so as to not be affected by the camera keyframes.\n\nBackground camera keyframed to ever-so-slightly move up and down upon impact.\n\n![camera_keyframe](assets/content/RunningDog/img/camerakeyframe.jpg)\n\n![camera_keyframe_gif](assets/content/RunningDog/img/camera-keyframe.gif)\n\nThis is suuuuuper rough (bg timing is not very good either and should be fixed..).. Let's make this better and start sketching the dog :D\n\n![doglayer](assets/content/RunningDog/img/doglayer.jpg)\n\nImma start with the head..\n\n![dogheadske](assets/content/RunningDog/img/dogheadske.gif)\n\nWhen the dog lands, it'll crouch a bit so the head will be lower than it is now on those frames but for now, lets get the general movement down before worrying about that.\n\nAdding the neck guidelines / sketch:\n\n![dogheadneck](assets/content/RunningDog/img/dogheadneck.gif)\n\nAdding the body (mainly rib placement atm)\n\n![dogheadneckbod](assets/content/RunningDog/img/dogheadneckbod.gif)\n\nAdding the tail.. this one was a bit complicated.. had to fiddle with it for a bit..\n\n![dogtail](assets/content/RunningDog/img/dogtail.gif)\n\nOMG.. dogs are hard to draw lmaooo (not rly I just need more practice to get good at it XD)\n\nLooked up a dog running gif on the internet for some reference..\n\nOk.. I think Imma just stylize the legs instead of making them look realistic.. maybe gonna give him \"hoof-like\" legs instead cuz I'm too lazy to learn how to draw a dog just for this one critique XD\n\n![doglegz](assets/content/RunningDog/img/doglegz.gif)\n\nAight, added the ears and snout..\n\n![runningdogskebgless](assets/content/RunningDog/img/runningdogskebgless.gif)\n\nChecking how it looks with the bg:\n\n![runningdogfullske](assets/content/RunningDog/img/runningdogfullske.gif)\n\nPretty good so far..\n\nI decided that it would be easier to just draw the dog lineart on a different animation layer than the sketch just in case I add more in betweens so made another layer for lineart.\n\n![dognewlayer](assets/content/RunningDog/img/dognewlayer.jpg)\n\nI will most likely color in the same animation layer (not same layer as the lineart but think of EACH frame in the animation layer as a folder and within that folder there are many more layers like lineart, color, etc.)\n\nSwitching gears, I decided to work on the background and created a new 1920 x 1080 CSP file. I then changed the canvas size so as to make it super long horizontally so as to make a revolving background. remember that the rightmost and leftmost section of your bg would need to be \"connectable\" with each other to sell that continuous effect when looping.\n\nI also expanded the canvas slightly vertically so as to maintain enough space for the camera's slight vertical pan when the dog jumps.\n\n![bgdoc](assets/content/RunningDog/img/bgdoc.jpg)\n\nI started with the clouds. \n\n![cloudline](assets/content/RunningDog/img/cloudline.jpg)\n\nI don't think there is wind strong enough to really make the clouds move drastically in any direction without it blowing the dog away (Clouds usually move rather slowly.. so unless the camera is focused on the clouds, you probs won't see it move much if at all.) so I'll keep the clouds static and have it wrap around in a revolving loop later.\n\n![cloudske](assets/content/RunningDog/img/cloudske.jpg)\n\nWhat I mean by revolve is if you take the right and leftmost cloud and you put them together, they should line up perfectly...\n\n![cloudwrap](assets/content/RunningDog/img/cloudwrap.jpg)\n\nThis is so that when you pan, you start with lining up the right side of this image to the right edge of the dog anim, and then you pan it by dragging it to the right slowly and smoothly. By the time it reaches the right side, there is no more art to continue it, you wrap it around and keyframe the image from its starting position coming in from the left..\n\nIt'll probably make more sense to see it in action so here's a video I made... (Keep in mind this is still quite rough... but it gets the point across..)\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Z0TCPKRYMY8?si=BFuFJLrWPrSQyyHu\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nMoving on, I added some \"mountains\" in the bg as well as some bushes.\n\n(Ignore the bad bush/nature art XD I'm just half assing the bg rn since this critique is mainly about making the bg feel like it is moving properly and making the whole thing feel like its moving cohesively.)\n\nI then added some (badly drawn) grass that is supposed to go in front of the dog and the bushes. I'll animate the grass for maybe like 3-4 frames for a quick gentle sway, color it, then copy and paste it across the screen.\n\n![grass](assets/content/RunningDog/img/grass.jpg)\n\nThe dog's animation layer will go between the grass and the bushes.\n\nSo the grass will cover the dog when they overlap and the dog will cover the bushes when they overlap.\n\nAdditionally, this is the order of animation layers from moving the fastest to slowest in terms of panning:\n\n1. Grass (Fast when compared to the others but not SUPER fast either..)\n2. Dog (about the same speed as the grass)\n3. Ground/Bushes (slower than the dog)\n4. Mountains (slower than the bushes)\n5. Clouds (the slowest)\n\nHere is the badly animated grass (Ok enough for the critique tutorial.. def not up to par for my personal work tho lol)\n\n![badgrassanim](assets/content/RunningDog/img/badgrassanim.gif)\n\nHere it is colored:\n\n![grasscolored](assets/content/RunningDog/img/grasscolored.gif)\n\nHere it is with the grass strewn across the page.\n\n![grassfullanim](assets/content/RunningDog/img/grassfullanim.gif)\n\nAnd because we are only seeing parts of it at a time, it won't seem too repetitive.. otherwise ideally one should draw different strands of grass throughout at least half of it or draw a few chunks of grass and mix up the order for variety - but even like this it seems alright.. (if you don't look too closely)\n\nI had the grass in a different file so I straight up copied the grass animation folder into the running dog csp file.\n\nI have the already animated grass layer above the dog running layer and keygramed the grass layer like so:\n\n![dogAndGrassKeyframe](assets/content/RunningDog/img/dogAndGrassKeyframe.jpg)\n\nAnd now it looks like this:\n\n![dogAndGrass](assets/content/RunningDog/img/dogAndGrass.gif)\n\nThis is rough.. but will probs refine it later.\n\nColored and \"animated\" the clouds via transform and keyframes.\n\nNote - this is one of the reasons why I prefer to NOT bake in my composite...\n\nAs you can see, the clouds are basically forced to move at the same speed as the grass and it looks unnatural.. This is because the rest of the animation is within the 27 frames I have here. If I were to expand the number of frames, the dog and grass would be empty or the cutoff would be weird.\n\nBUT when compositing/video editing after exporting it out, you have a LOT more control over the individual elements and how fast or slow they move.\n\nYou can have the clouds move much slower and just keep looping the dog and the grass via copy and paste once you have the first loop down and make the duration long enough to cover the duration of the clouds. Or if it somehow never matches up, you can still do a few loops and then do a slow fade out so it doesn't look awkward. (I will demo that later)\n\n![mehclouds.gif](assets/content/RunningDog/img/mehclouds.gif)\n\n![mehclouds.jpg](assets/content/RunningDog/img/MehClouds.jpg)\n\nAdded the bg and the ground.. though as you can see, if the ground is static, it would look very strange and out of place as everything else is moving.\n\n![runningdogstaticfloor.gif](assets/content/RunningDog/img/runningdogstaticfloor.gif)\n\n![runningdogmovingfloor.gif](assets/content/RunningDog/img/runningdogmovingfloor.gif)\n\nNot sure why the bg is so choppy like that.. but we can probs fix that in compositing.. I think that's part of why I prefer exporting and then compositing it otherwise you get weird artifacts like this. (Could be a CSP thing though.. not sure actually...)\n\nIn any case, as you can see, the floor being moved along like a treadmill adds to the movement.\n\nThe clouds really bother me tho.. can't wait to fix them when I composite XD\n\nMoving on.. I fininshed the floor and the bushes and ended up putting them on the same layer.. I like how it came out.\n\nHere is a PNG of it.\n\n![bushesAndGround.png](assets/content/RunningDog/img/bushesAndGround.png)\n\nI don't like the mountains tho.. but that's cus I didn't try too hard since I have other projects to work on and this project was taking longer than anticipated XD But it's been fun!\n\n![mountains.png](assets/content/RunningDog/img/mountains.png)\n\nAs I was experimenting, I found that layering the mountains and having the layers move at different speeds (further ones moving slower, closer ones moving faster) makes it look better.\n\n![runningDogWithBg.gif](assets/content/RunningDog/img/runningDogWithBg.gif)\n\nHowever, I still don't like how hard it is to time everything (when you have a lot of moving pieces like this) all within CSP. This would be much easier to do with say, Da Vinci Resolve, Adobe Premiere Pro, or some kind of video editor so that everything can have a different speed.\n\nRandom note: I shaded the grass a little but but because of how I did the keyframes, its too much work to implement it into the csp ver so I will use it in the Compositing ver.\n\nFor now, just know that the grass now looks like this:\n\n![shadedGrass.png](assets/content/RunningDog/img/shadedGrass.png)\n\nThough you probably can't tell from far away. (But don't worry, it'll be enlarged during the compositing phase.)\n\n![shadedGrass.gif](assets/content/RunningDog/img/shadedGrass.gif)\n\nSo now my background layers are all done (though in a real project I would add a lot more shading first) and consists of the following Layers (ordered by furthest to closest):\n\n1. Sky\n\n![sky.jpg](assets/content/RunningDog/img/sky.jpg)\n\n2. Clouds\n\n![clouds.png](assets/content/RunningDog/img/clouds.png)\n\n3. Mountains\n\n![mountains.png](assets/content/RunningDog/img/mountains.png)\n\n4. Ground / Bushes\n\n![bushesAndGround.png](assets/content/RunningDog/img/bushesAndGround.png)\n\n5. Grass\n\n![GrassNoBg.gif](assets/content/RunningDog/img/GrassNoBg.gif)\n\nAs you can see, only the Grass is actually animated.. the rest are all \"animated\" by literally transforming and moving the images around.\n\nNow, back to the badly drawn dog (didn't use references for most of it thats why).... \n\nYes.. Yes, I know, there are many things wrong with this dog animation (since I'm not really trying very hard with this dog...)\n\nAight.. here's the lineart of the dog:\n\n![runningdoglineart.gif](assets/content/RunningDog/img/runningdoglineart.gif)\n\nAnd here is the lineart with the shadow (also roughly done...):\n\n![runningdogwithshadow.gif](assets/content/RunningDog/img/runningdogwithshadow.gif)\n\nHere, I like to do a rough check of the lineart by watching it loop while focusing on different parts of the animation.\n\nWatch the loop and focus on just the tail and check if it looks ok.\nThen observe just the back legs, then just the front legs, then just the torso, then just the eyes, then just the tongue, and.. you get the idea..\n\nBy only observing PARTS of the animation at once, it helps you better spot the looping errors - especially if part of the animation seems to not be very smooth but you can't figure out exactly where. (For example, the fur on the nape of the dog's neck appears and disappears too suddenly. I should probably add a few more frames where the fur eases in and out for better flow.)\n\nExample: \n\nFocusing on the Ears:\n\n![Ears.gif](assets/content/RunningDog/img/Ears.gif)\n\nTongue:\n\n![Tongue.gif](assets/content/RunningDog/img/Tongue.gif)\n\nForelegs:\n\n![Forelegs.gif](assets/content/RunningDog/img/Forelegs.gif)\n\nBack Legs:\n\n![BackLegs.gif](assets/content/RunningDog/img/BackLegs.gif)\n\nTail:\n\n![Tail.gif](assets/content/RunningDog/img/Tail.gif)\n\nAdded some gentle/light baked in comp.. \n\nJust a tiny bit of atmospheric lighting + contrast on top of everything.\n\n![bakedInComp.jpg](assets/content/RunningDog/img/bakedInComp.jpg)\n\nHere is what it looks like without the baked in comp:\n\n![runningDogNoComp.gif](assets/content/RunningDog/img/runningDogNoComp.gif)\n\nAnd here is what it looks like with the comp:\n\n![runningDogBakedInComp.gif](assets/content/RunningDog/img/runningDogBakedInComp.gif)\n\nAs you can see, the gradient lighting and added contrast gives it more depth.\n\nNow, I could go further and add and lighting shading on everything individually (and most of the time I do but I'll leave it like this for this one since its not a serious project...)\n\nLaid down some color flats. (Orig I made the dog darker but the value was too close to the bg so I made him a lighter gray.)\n\n![runningDogFlats.gif](assets/content/RunningDog/img/runningDogFlats.gif)\n\nThe background still seemed too flat so I added a bit more comp for the baked in version with different gradients clipped onto the different layers of bg using different blending modes like so:\n\n![moreComp.jpg](assets/content/RunningDog/img/moreComp.jpg)\n\nDid a value check and it looks alright... In a more serious project I would pay a lot more attention to making the values contrast a lot more.. But since this is not the focus of this critique.. I'll settle with this for now.\n\n![runningdogValueCheck.gif](assets/content/RunningDog/img/runningdogValueCheck.gif)\n\nAnd Here's the colored ver.. Adding all that comp makes it look a lot more vibrant and less flat imo.\n\n![runningDogMoreComp.gif](assets/content/RunningDog/img/runningDogMoreComp.gif)\n\nOne thing I noticed is - if you want the clouds to be directly above the dog, then there will also need to be shadows for the clouds on the floor and the dog.\n\nBut I want it to be more in the bg so will probably do some kind of blur on it later so its not quite as sharp. Otherwise everything looks more or less about the same distance due to everything having such sharp lineart/color.. But I can't do that in CSP (as far as I know) since the blur effect only affects singular layers not folders. \n\nThat would be another good reason to comp things in a video editor so you can have more control over stuff like that.\n\nOk! Finally finished coloring the dog:\n\n![runningDogFullColor.gif](assets/content/RunningDog/img/runningDogFullColor.gif)\n\nAnd added some more comp just cus it didn't look quite right.. I dimmed the bg/mountains just a bit just so you can see the dog better.\n\n![andSomeMoreComp.jpg](assets/content/RunningDog/img/andSomeMoreComp.jpg)\n\nHonestly still doesn't look quite right.. but its ok we are going to make it better during composting in this next section!\n\n![runningdogWithBakedInComp.gif](assets/content/RunningDog/img/runningdogWithBakedInComp.gif)\n\n---\n\n### Compositing in Da Vinci Resolve\n\nActually, the reason why I wanted to do this critique/tutorial was cuz I wanted to get back into animating after not having done any serious animation for like 1.5 years, AND also because I wanted to learn how to composite better in Da Vinci Resolve.\n\nSo for this part, we are sort of going to be learning together!\n\nNote: You don't have to use Da Vinci Resolve if you already have a video editor you like to use. Compositing concepts are probably going to be roughly the same.\n\nIn Da Vinci, most of the fancy stuff is done in Fusion.\n\nFusion uses a thing called nodes.\n\nThis is the video that helped me figure out, a few years back, how to actually use nodes in fusion. The only one that really made a lot of sense to me and made things click. (Though this is an updated video the content is more or less the same.)\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PoinMs_YceQ?si=WD21TBAGAjaO5SqH\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nSo first, I'm going to export my backgrounds, uncomp'd!\n\nBy that I mean do it one background item at a time (one for the mountain, one for the ground/trees, one for the clouds, one for the grass.)\n\nSo its not going to be one lump of all the backgrounds together. Going to need each item separately so we can apply different comps to each of the different items.\n\nI also exported the dog without the shadow as a png sequence, and exported just the shadow as a separate png sequence so in case we want to do something fancy with just the dog or just the shadow, they are not bound to each other.\n\nThe grass is also a png sequence.\n\nSo my files look like this:\n\n![compFiles.jpg](assets/content/RunningDog/img/compFiles.jpg)\n\n![dogCompFiles.jpg](assets/content/RunningDog/img/dogCompFiles.jpg)\n\n![dogShadowCompFiles.jpg](assets/content/RunningDog/img/dogShadowCompFiles.jpg)\n\nIn Da Vinci Resolve, I started a new project and went to the Edit tab\n\n![editTab.jpg](assets/content/RunningDog/img/editTab.jpg)\n\nThen I imported all my assets:\n\n![importedAssets.jpg](assets/content/RunningDog/img/importedAssets.jpg)\n\nI moved the Dog onto the timeline, made a new track and moved the shadow below the dog, lining them up.\n\n![timelineDogShadow.jpg](assets/content/RunningDog/img/timelineDogShadow.jpg)\n\nThat looks like this in the viewport.\n\n![viewportDogShadow.jpg](assets/content/RunningDog/img/viewportDogShadow.jpg)\n\nThis is a very basic set up but before we get into the fancy stuff, I'm going to watch a refresher video.\n\nAt the bottom of this post, I linked some tutorials on how to use Da Vinci Resolve if this is your first time.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OtKhezId7As?si=p2WGkntUlgb2a6K7\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nOk so after watching that tut, I have a better idea of how I want to composite this.\n\n![comp-setup.png](assets/content/RunningDog/img/comp-setup.png)\n\nWhat I would like to do though is also add a lightning effect so its like the sun is coming from the right side... but with all these being separate clips and they are all animated at different speeds, puting them in one clip would not be very flexible... I remembered there is a way to add something on top of everything else by making a new fusion composition.\n\nSo I right clicked in the media pool and clicked New Fusion Compositon:\n\n![addFusionComp.png](assets/content/RunningDog/img/addFusionComp.png)\n\nClicked Create:\n\n![newFusionComp.jpg](assets/content/RunningDog/img/newFusionComp.jpg)\n\nThen dragged the new fusion composition into the timeline ABOVE everything else, as the lighting should be above all the layers.\n\n![moveFusionComp.jpg](assets/content/RunningDog/img/moveFusionComp.jpg)\n\nOp and now there is a warning that there is no frame available for media out. What that means is basically the only node in there is media out and you need to have something for it to output.\n\n![FusionCompWarning.jpg](assets/content/RunningDog/img/FusionCompWarning.jpg)\n\nSo with the Fusion Composition clip selected, I went to the Fusion Tab:\n\n![GoToFusionTab.jpg](assets/content/RunningDog/img/GoToFusionTab.jpg)\n\nAs you can see, there is just one lonely Media Out node all by its lonesome...\n\n![lonelyMediaOut.jpg](assets/content/RunningDog/img/lonelyMediaOut.jpg)\n\nLet's add a background as a placeholder for now. We can change it later.\n\nClick near the media out node and then click Command (Mac) or CTRL (Windows) + Spacebar.\n\nThat should bring up a Select tool.\n\nType in bg and add the Background\n\n![cmdCtrlSpace.jpg](assets/content/RunningDog/img/cmdCtrlSpace.jpg)\n\nNow you will see the background and media out nodes.\n\n![connectBgMediaOut.jpg](assets/content/RunningDog/img/connectBgMediaOut.jpg)\n\nConnect them like so:\n\n![BgMOConnected.jpg](assets/content/RunningDog/img/BgMOConnected.jpg)\n\nNow go back to the edit/timeline page and you will notice that the error is now gone.\n\n![NoMoreWarning.jpg](assets/content/RunningDog/img/NoMoreWarning.jpg)\n\nAs the background by default is black (you can change the color but this is just a placeholder for now), if you lower the opacity you can still see the stuff under it, affecting ALL the layers, thus confirming that it is doing what we want it to do.\n\n![OpacityCheck.jpg](assets/content/RunningDog/img/OpacityCheck.jpg)\n\nNow, back to the fusion tab, I clicked on the background and changed the type from Solid to Gradient. For now, I selected some kind of yellow for the rightmost color:\n\n![BGgradient.jpg](assets/content/RunningDog/img/BGgradient.jpg) \n\nNow, when it comes to the leftmost one, you might wonder why there are no colors for you to select like the one on the right. No worries, just drag the color bar from black to white:\n\n![BGgradient2.jpg](assets/content/RunningDog/img/BGgradient2.jpg) \n\nNow, you will have the full array of colors to select from. I opted for a cyan-ish color for now... just to see how it looks.\n\n![gradientCyan.jpg](assets/content/RunningDog/img/gradientCyan.jpg)\n\nI also added a color in the middle by clicking that position, and put it at a pastel yellow, closer to white. then lowered the alpha a bit to make it sort of transparent since I don't want the middle to stand out that much.\n\n![lowerAlpha.jpg](assets/content/RunningDog/img/lowerAlpha.jpg)\n\nNow, going back to the EDIT Tab, on the right side, I fiddled with it and decided to change composite mode to Divide.. You should toggle through all the settings under composite mode and see which one works best for you.\n\nI also lowered the opacity for it as well..\n\n![Divide.jpg](assets/content/RunningDog/img/Divide.jpg) \n\nSince Divide made it so the left side is pink and the right side is blue, I used the flip button to flip it around so that the right side is pink and the left side is blue.\n\n![Flip.jpg](assets/content/RunningDog/img/Flip.jpg) \n\nFiddled around and added another gradient color for transition:\n\n![moreGradient.jpg](assets/content/RunningDog/img/moreGradient.jpg)\n\nAfter fiddling some more, I felt that it looked better with Opacity at or near 100% so I set it back to 100% for now...\n\n![overview.jpg](assets/content/RunningDog/img/overview.jpg) \n\nNow, if you are familiar with Da Vinci, you might be wondering... why aren't you using the color tab for these color corrections?\n\n(For those that aren't familiar, the color tab is super cool - I linked some videos at the very bottom of the blogpost so make sure to check em out!)\n\nThe reason I'm not using the color tab at the moment is because right now I have a bunch of layered clips and if you want to color grade EVERYTHING TOGETHER, it needs to be one clip.. Otherwise it'd be the same as doing everything individually which defeats the point for what I'm trying to do here BUT once everything is set up and composited and the loops are set as I'd like, I can create a composite clip that basically turns all the layers into ONE singular clip.\n\nPersonally, it is at THAT point that I would use the color tab for the finishing touches. So for now, that Fusion Comp Gradient Layer I'm putting on top is more or less a placeholder for now. If I like it I'll keep it but I'll probs be making more color grading touches in the Color tab.\n\n![colorTab.jpg](assets/content/RunningDog/img/colorTab.jpg) \n\nOne thing I was concerned about is whether or not a shorter clip can keep looping while the fusion clip was longer.. and I found out that yes, it can!!\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vs08wbx5t3c?si=0n4z-SpmvibSKNg1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nFrom here on out, I'll just record my process as writing down what I'm doing step by step takes too long. \n\nI ended up deleting that one fusion layer I had on top of everything else since I fiddled with making a new fusion layer and putting ALL the images and clips into one fusion layer but the issue was that I needed to be able to move the background and foreground up and down when the dog landed on impact but it was hard to do with the nodes as the dog sat between the grass and the background thus if I moved the background and the foreground, the dog would also move - so I ended up breaking it out.\n\nIf you want to skip my fiddling around, jump to the 10 minute mark in the video.\n\nSo while putting everything into one fusion layer is a good technique it doesn't quite work out for me in this case. You may run into stuff like this when you composite too, but just roll with it and have fun and make tweaks and changes where needed!\n\nHere is the full timelapse of my 7.5 hours of compositing but sped up 700+% so that it's only 1 hour long. Feel free to slow down the vid on youtube.\n\nI was gonna add some subtitles, detailing what I did, but I got lazy so.. if you really want to know what I'm doing in a certain part, feel free to dm me.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4nSqu6nELME?si=4-2c-pHS6XEPE29r\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nThrough a few hours of trial and error of using different methods to composite, here is what I ended up with:\n\n![runningDogFinalCompressedGIF.gif](assets/content/RunningDog/img/runningDogFinalCompressedGIF.gif)\n\nHere is the High Res Ver on youtube (Though due to youtube playback and network speed, it may not load as smoothly as a gif.):\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/04N1HEK5PRU?si=CrW_dNedZp-5bpvv\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n---\n\n### Comping Takeaways\n\n1. Try different methods, and eventually you'll find what works for you. It's ok to spend a lot of time on method only to later choose a different method you were considering.\n\n2. If you have different layers of bg in separated clips and you want to move the whole background AND foreground BUT not the character sandwiched between, it appears to be easier to just composite each of the clips separately and then turn them into one composite clip to move them.\n\n3. If rendering or playback is slow, you probably need more ram. \n- Ways you can optimize your playback include:\n    - Go to DaVinci Resolve > Preferences > System > Memory and GPU and increase the memory limit as much as you can. If it is maxed out, nothing you can do there.\n    - Go to Playback > Render Cache > Set it to Smart or User and change Render Cache Fusion Output to On\n    - Try going to Playback > Timeline Proxy Mode > Half Resolution or Quarter Resolution. This makes your playback blurry but it will be fine when rendered.\n\n- Patience is key. If you have done all of the above and it still is laggy, just wait it out. Make yourself a snack or drink or finish some chores while you wait.\n\n- If playback is so laggy that you can't really tell the timing of things, (and this has happened to me before - a long forgotten memory til now...) render it out and watch the animation. \nThen when you go back, try to tweak it based on what you want to see corrected in the render version. Render again, and see how close you are and retweak as needed. Rinse and Repeat.\nWith experience, your tweaks will get more accurate and your compositing will take less time. (Been almost 2 years since I last composited so I totally forgot about this lag.)\n\n4. When trying to make something loop, first, set up the thing that will take the longest to play through. For example I know the clouds are going to move the slowest so everything else will need to LOOP Perfectly within the cloud's loop duration. If anything is off, it will feel choppy. So say you have the cloud down but the dog is off by 2 frames at the end. What you can do is make sure the dog gets the few frames added back AND extend the cloud's loop by just a bit, thus slowing it just down a little bit more - but keeping the balance.\n\n    IMO this is the hardest part about making a looping animation - timing everything so that all the smaller loops - ALL TIMED DIFFERENTLY - fit perfectly within the largest loop, and subsequently making sure the transition is SMOOTH for every single item in the animation.\n\n    Be Patient, this takes a LOT of tweaking and trial and error! But once you get it to loop perfectly it is super satisfying XD\n\n    Repeatedly play the last few frames of the animation and loop around to the beginning, checking everything to ensure that the transition is as smooth as possible.\n\n5. IF you can, set up the animation first before compositing otherwise playback will be pretty slow.\n\n6. Instead of using clips and copying and pasting them if they aren't long enough to match the full loop, simply make a fusion clip and drag the small loop in as the media in, connect it to the media out, hit loop and when you go back to Edit, you can make it as long as you need to and it will still loop!!\n\n    HOWEVER - Note that when you make the clip longer than it already is, you may see an error saying no media found. Just go back into the fusion clip, uncheck and RE-check the loop checkbox and go back to Edit and it will show up again. My guess is that either it takes time to sync the loops or it just can't tell that it's been expanded and thus is confused so unchecking and rechecking the loop box basically reorients it.\n\n7. Use markers to mark where something is supposed to happen. For example I used red markers (you can pick any color) to mark when the dog lands so that I can make sure when the whole background does the camera shake, that it doesn't do so off beat and that it is synced properly.\n\n---\n\n### Critique of my own animation:\n\n#### Baked In Comp Version:\n\n![runningdogWithBakedInComp.gif](assets/content/RunningDog/img/runningdogWithBakedInComp.gif)\n\n1. Bg is too colorful, dog is too gray, focus is more on bg... But if the bg is too boring, the whole thing looks quite dull cus the dog is already gray..\n\n2. Pretty much everything is timed at the same speed... doesn't look natural. I like each of the elements individually.. but together.. they seem a bit busy - possibly because everything is moving at the same speed so this may not be an issue after Da Vinci Comp is done..\n\n3. Dog could be drawn better...... the conversion from dog legs to block legs look a bit weird but the grass hides it XD\n\n4. The ambient lighting looks alright but there are a few artifacts maybe due to how CSP exports gifs?\n\n5. I personally would rate myself a 5/10 here.. its aight.... meh.\n\n#### Da Vinci Resolve Comp Ver:\n\n![runningDogFinalCompressedGIF.gif](assets/content/RunningDog/img/runningDogFinalCompressedGIF.gif)\n\n1. The cloud could have been connected better on the loop but oh wells..\n\n2. There is a faint horizontal line on the closer mountain.. Couldn't figure out why tho..\n\n3. It was a lot harder to do the camera shake when the dog lands.. I feel like there is a better way of doing this in Da Vinci but I haven't figured out what it is yet..\n\n4. Overall I'm quite satisfied that everything loops quite nicely and pretty much seamlessly!\n\n5. Lighting turned out decent!\n\n---\n\n### Time It Took:\n\nBaked In Comp: Roughly 2-3 weeks (Ranging from 1-3 hours of work on the weekdays and 8-16 hours on the weekends)\n\nExported Then Comp'd: Roughly 3-4 weeks (Ranging from 1-3 hours of work on the weekdays and 8-16 hours on the weekends)\n\nThough it took a bit longer than it would usually take for me also because I was screenshotting and documenting everything.\n\n---\n\n### Additional Resources:\n\nGood Vid on Animating 2D Backgrounds Dynamically: \n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/M7yirNAyD5M?si=kpriD38mOgQhht05\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nDa Vinci Resolve Tutorials (General):\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qDHnCFMZ9HA?si=ezFk65wt16mna2pT\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pwg8D0P4z7M?si=Sts6Enp919ZVbnkd\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nDa Vinci Resolve Color Grading/Correction Tuts:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SkosqJfzEs0?si=SKTVA_QGViZUKvGL\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YfyriApWIJY?si=xUizx6PIpZDsjjsh\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n---\n\n#### If you read this far, thanks for reading and I hoped it was helpful and hope you enjoyed it! :D\n\n","folderName":"RunningDog"}]